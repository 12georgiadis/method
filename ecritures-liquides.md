## Liquid Writing

On porous boundaries between writing, shooting, and editing.

---

The phrase "liquid writing" is not a metaphor. It describes what actually happens when generative AI enters a filmmaking practice: the sequential logic of cinema production dissolves. You don't write, then shoot, then edit. You do all three at once, and each gesture contaminates the others.

A text becomes an image. An archive mutates. A shot morphs into something the camera never captured. The transitions between these states are no longer discrete steps -- they're continuous flows. The screenplay doesn't precede the film. The film doesn't wait for the edit. Everything is in motion simultaneously, and the AI is what makes this simultaneity possible.

This is not a new idea. Chris Marker worked this way with text, image, and memory. Gregory Chatonsky has been exploring generative flows for decades. What's new is the bandwidth. The speed at which a thought can become an image, an image can become a sequence, a sequence can feed back into a thought. The loop tightens until the distinction between conception and execution becomes meaningless.

---

### Four axes

Liquid writing operates along four axes. These are not categories of work -- they're tensions that coexist in every project.

**Hybridization of the real.** Generative AI reconfigures documentary materials without replacing them. Archives, testimonies, captured images pass through generative models and come out transformed -- altered, chimeric, somewhere between the recorded and the synthesized. The real remains at the center of the process. The AI reconfigures its forms and narratives, but the starting point is always something that existed, something that happened, someone who lived.

This is the opposite of generating fictional worlds from scratch. The latent space of a diffusion model encodes statistical patterns learned from millions of images -- not every image ever made, but a vast, biased, incomplete compression of visual culture. When you feed a documentary image into that space, it doesn't disappear. It encounters these patterns, these accumulated traces of other images. The result is a chimera: part document, part statistical artifact, part collective visual memory.

**Morphogenesis as writing.** The visual glitches, the chimeras, the artifacts produced by diffusion models are not bugs to fix. They are expressive materials to invest. The zones of transformation -- where a face becomes a landscape, where a body dissolves into texture, where two incompatible images merge -- are a new visual language.

This language emerges from the dialogue between artistic intention and model behavior. You can push the model toward something, but you can't fully control what happens in the transition. That space of partial control is where the most interesting images live. It's comparable to what Peter Tscherkassky does with celluloid, or what Francis Bacon did with paint -- working at the edge of the medium's material behavior.

**Bias as critical material.** AI models are not neutral instruments. Their tendencies, their refusals, their distortions are ideological and aesthetic choices made visible. The sycophancy of models -- their systematic drive to please -- produces images that are flattering, smooth, culturally normative. This is not a flaw to work around. It's a subject to work with.

When a model consistently turns female figures into fashion magazine silhouettes, or bathes every scene in golden-hour light, or produces what I call a "Geo magazine" aesthetic -- that reveals something about the training data, about the attention economy, about what "beautiful" means when it's defined by engagement metrics. Making these biases visible and putting them to work critically is part of the artistic process. (More on this in [Sycophancy](sycophancy.md).)

**Intimate/collective dialogue.** Fine-tuning and LoRA adapters on personal corpora -- rushes, family archives, research notebooks -- create models that "know" an author's universe. RAG anchors model responses in specific documents. Context engineering structures conversations that can accompany writing over time. These techniques create a productive tension between the personal and the generic, between the intimate and a model trained on the world.

The question is not "can the AI understand my work?" It can't. The question is: what happens when my personal archive meets the statistical average of all human visual production? The collision between these two scales -- the singular and the planetary -- is where something unexpected can emerge.

---

### Not a replacement

AI is a layer in the creative process, not a substitute for existing practices. Every project articulates generative tools with what already exists: writing, shooting, editing, performing.

Four modes of articulation:

- **Previsualization.** Materializing visual intentions before shooting. Creating a space for rapid, iterative research between the idea and its concretization. The director and cinematographer can explore ten variations of a scene in the time it takes to set up one real shot.
- **Post-production material.** Transforming rushes through generative models: alteration, hybridization, recomposition. The captured image becomes a starting point rather than a final result.
- **Performance instrument.** Real-time or near-real-time generation integrated into filmed performance, where the AI becomes a partner on stage, captured by the camera. This is where filmmaking meets the living arts.
- **Final image.** Beyond the tool, generative AI produces images that *are* the work: assumed deepfakes, blends of animation and photorealism, visual chimeras. This engages a reflection on the renewal of representation in art history and the reconfiguration of the real through generated images.

These approaches belong to a practice where the boundaries between writing, shooting, and editing are no longer sequential stages but simultaneous flows. That's what liquid writing means.

---

[Back to Method](README.md)
